{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DUzESq5OyOT_",
        "outputId": "eea227e9-b9ff-4a8c-855d-7aef7a5bcd73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat May 10 06:02:57 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   66C    P8             11W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# check the gpu type and information\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install the transformers"
      ],
      "metadata": {
        "id": "KOCZ-zmjykbs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "qgL8dsSFygm1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import the pipeline from transformers\n",
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "oki8SapFypic"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1) sentiment analysis example"
      ],
      "metadata": {
        "id": "LhGXdV0Iy_80"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# take a sample text of negative review and guess what will be the output\n",
        "sample_text = \"Hey! I am watching this movie and you recommended it to watch but my guess is that this movie is not worth watching and i also suggest you to not\"\n",
        "generator = pipeline(\"text-generation\", model = \"distilbert/distilgpt2\")\n",
        "result = generator(sample_text, max_length = 100, num_return_sequences = 2)\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lxGVZGwjy1yN",
        "outputId": "135d248d-307a-4f72-a229-4774313bccba"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': 'Hey! I am watching this movie and you recommended it to watch but my guess is that this movie is not worth watching and i also suggest you to not watch it for other reasons but to watch it for all you dont want and i do it for you no one wants it to happen and i promise no one gives a fuck about to watch it and I am not happy with the ending and I will never give the fuck about that. (lol and i love the music but you got to go out'},\n",
              " {'generated_text': 'Hey! I am watching this movie and you recommended it to watch but my guess is that this movie is not worth watching and i also suggest you to not watch this. Your browser does not support HTML5 video tag.Click here to view original GIF\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'}]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### result: the model has printed a negative review with a score of 99% which means that the model is performing very well"
      ],
      "metadata": {
        "id": "VfwdF4Hezxkl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### QA generation"
      ],
      "metadata": {
        "id": "2noGM1BZ1TPV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "q_classifier = pipeline(task = \"question-answering\", model = \"deepset/roberta-base-squad2\") # here model is also provided\n",
        "t_classifier = pipeline(task = \"text-generation\", model = \"distilbert/distilgpt2\")\n",
        "question = \"Who will win the war? Pakistan or India\"\n",
        "context = \"India attacked pakistan in May 2025\"\n",
        "q_result = q_classifier(question = question, context = context)\n",
        "t_result = t_classifier(question, max_length= 100)\n",
        "print(f\"Answer of the Question asked is --> {q_result}\")\n",
        "print(f\"Text of the question asked is --> {t_result}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rw6n_iwIznrO",
        "outputId": "c564fb9a-68c9-4cce-e7ca-d10375cd937c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer of the Question asked is --> {'score': 0.05947365611791611, 'start': 0, 'end': 5, 'answer': 'India'}\n",
            "Text of the question asked is --> [{'generated_text': 'Who will win the war? Pakistan or India are going to have to be a bad force. India is going to go too far. We need to protect our borders and our friends and even our enemies in Central and Eastern Asia (particularly those in the Greater India Sea). We will have to help keep it out of the hands of people who come to the United States, to protect our allies in the region.\"\\n\\n\\nIn addition, Pakistan\\u200f has been in India for as long as five'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ZVCrcXEr23_G"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}